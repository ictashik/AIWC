{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0069411e",
      "metadata": {},
      "source": [
        "# Day 3: Convolutional Neural Networks (CNNs) for Medical Imaging\n",
        "\n",
        "Welcome to **Day 3** of our elective: **AI in White Coat**. Today, we’ll dive into **Convolutional Neural Networks (CNNs)** using **PyTorch**, focusing on the **ChestMNIST** dataset from [MedMNIST](https://medmnist.com/). CNNs are the foundation of many **state-of-the-art** medical imaging applications, such as:\n",
        "\n",
        "- Automated X-ray classification (normal vs. pneumonia).\n",
        "- Detection of lesions or tumors on CT/MRI.\n",
        "- More advanced tasks like segmentation and object detection.\n",
        "\n",
        "By the end of this notebook, you’ll:\n",
        "1. Understand the basics of CNN layers and why they’re well-suited for images.\n",
        "2. Use a *prompt-first* approach to set up a simple CNN in PyTorch.\n",
        "3. Train and evaluate the CNN on the **ChestMNIST** dataset.\n",
        "4. Observe how CNNs can outperform simple models on image tasks.\n",
        "\n",
        "---\n",
        "## 1. Recap & Prerequisites\n",
        "So far:\n",
        "- **Day 1**: HPC setup, logging in, environment checks, prompt-first approach.\n",
        "- **Day 2**: Basic ML classification with logistic regression on **BloodMNIST**.\n",
        "\n",
        "Today, we’ll leverage **PyTorch** for a deeper neural network approach. Make sure you have PyTorch installed on the HPC. If not, ask an LLM or your mentor for the correct install commands.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1e065b",
      "metadata": {},
      "source": [
        "## 2. PyTorch Installation Check\n",
        "\n",
        "If PyTorch isn’t already installed, we can install it with:\n",
        "```\n",
        "!pip install torch torchvision\n",
        "```\n",
        "But let’s do a quick check first. If you’re missing anything, prompt your LLM for an install command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8239c4b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Environment Check for PyTorch ======\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA is available! GPU ready.\")\n",
        "    else:\n",
        "        print(\"No GPU detected. Training might be slower on CPU.\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch not found. Please install via pip or conda.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19dbbb16",
      "metadata": {},
      "source": [
        "## 3. About the ChestMNIST Dataset\n",
        "\n",
        "**ChestMNIST** is part of **MedMNIST**, containing chest X-ray images labeled as **one of five classes** (normal lung, lung opacity, etc.). Each image is 28x28 pixels, grayscale (similar to classic MNIST style, but medical!).\n",
        "\n",
        "- Dataset link: [Hugging Face: MedMNIST/ChestMNIST](https://huggingface.co/datasets/MedMNIST/chestmnist)\n",
        "- Typical splits: train, val, test\n",
        "- Task: multi-class classification (if the data is set up that way) or multi-label classification depending on version. For simplicity, we’ll treat it as multi-class.\n",
        "\n",
        "### Prompt Example:\n",
        "```\n",
        "I'm working in a Jupyter notebook with PyTorch.\n",
        "Please generate code to load ChestMNIST from 'MedMNIST/chestmnist'\n",
        "using the 'datasets' library, then show me how to explore it.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d4700c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Load ChestMNIST ======\n",
        "from datasets import load_dataset\n",
        "\n",
        "chestmnist = load_dataset(\"MedMNIST/chestmnist\")\n",
        "print(chestmnist)\n",
        "\n",
        "# Let's peek at one sample from the train split.\n",
        "sample = chestmnist['train'][0]\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86894d74",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing & Dataloaders\n",
        "\n",
        "In PyTorch, we typically create **Datasets** and **Dataloaders** to handle batching, shuffling, etc. For image data:\n",
        "1. Convert each image to a **torch.Tensor**.\n",
        "2. Normalize or scale pixel values if needed (e.g., from `[0, 255]` to `[0, 1]`).\n",
        "3. Use `torch.utils.data.DataLoader` to batch and shuffle.\n",
        "\n",
        "### Prompt Example:\n",
        "```\n",
        "Please generate PyTorch code to:\n",
        "1. Convert the 'train' and 'test' splits from chestmnist into torch Datasets.\n",
        "2. Create DataLoaders with a batch size of 32.\n",
        "3. Return (image_tensor, label_tensor) pairs.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2389d3f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== LLM-GENERATED CODE CELL: Create PyTorch DataLoaders ======\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ChestMnistDataset(Dataset):\n",
        "    def __init__(self, split_data):\n",
        "        self.data = split_data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # image: 28x28 grayscale\n",
        "        image = torch.tensor(item['image'], dtype=torch.float32)\n",
        "        # Expand dims to [1, 28, 28] for a single-channel image\n",
        "        image = image.unsqueeze(0)\n",
        "        # label may be multi-class; if it's just one int, treat as single class\n",
        "        label = torch.tensor(item['label'], dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# Prepare train, validation, and test sets\n",
        "train_ds = ChestMnistDataset(chestmnist['train'])\n",
        "# Check if there's a validation split\n",
        "val_ds = None\n",
        "if 'validation' in chestmnist:\n",
        "    val_ds = ChestMnistDataset(chestmnist['validation'])\n",
        "test_ds = ChestMnistDataset(chestmnist['test'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = None\n",
        "if val_ds:\n",
        "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a940c979",
      "metadata": {},
      "source": [
        "## 5. Defining a Simple CNN in PyTorch\n",
        "\n",
        "A typical CNN has:\n",
        "- **Convolution layers**: Extract spatial features.\n",
        "- **Pooling**: Downsample the feature maps.\n",
        "- **Fully connected**: Final classification.\n",
        "\n",
        "We’ll do a small architecture with a couple of conv layers. **ChestMNIST** images are 28×28 grayscale, so input channels = 1.\n",
        "\n",
        "### Prompt Example:\n",
        "```\n",
        "Please generate a simple PyTorch CNN model for single-channel 28x28 images,\n",
        "with two convolutional layers, ReLU, and a final linear output.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e08035",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== LLM-GENERATED CODE CELL: Define CNN ======\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        # After two pooling operations, 28x28 -> 14x14 -> 7x7\n",
        "        # With 32 channels, that becomes 32 * 7 * 7 = 1568\n",
        "        self.fc = nn.Linear(32 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, 1, 28, 28]\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN(num_classes=5)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb474d1f",
      "metadata": {},
      "source": [
        "We assume there are **5 classes** in ChestMNIST (the exact count depends on the dataset version). Adjust if needed.\n",
        "\n",
        "## 6. Training Loop\n",
        "\n",
        "We’ll write a standard PyTorch training loop. High-level steps:\n",
        "1. Move data to GPU if available.\n",
        "2. Forward pass → compute loss → backprop → update.\n",
        "3. Track accuracy over the epoch.\n",
        "\n",
        "### Prompt Example\n",
        "```\n",
        "Please generate a PyTorch training loop for the SimpleCNN model,\n",
        "using cross-entropy loss and Adam optimizer. One or two epochs is enough for demo.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7484cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== LLM-GENERATED CODE CELL: Training ======\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 2  # We'll just do 2 for demonstration.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224c0673",
      "metadata": {},
      "source": [
        "## 7. Validation & Testing\n",
        "\n",
        "If **ChestMNIST** includes a validation set, we can evaluate it after each epoch. If not, we can go directly to the **test set**. We’ll show a basic test loop below.\n",
        "\n",
        "### Prompt Example\n",
        "```\n",
        "Please generate code to evaluate the trained CNN on the test_loader.\n",
        "Compute accuracy, print the result.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1037dfd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== LLM-GENERATED CODE CELL: Testing ======\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27d7dcf",
      "metadata": {},
      "source": [
        "Depending on the number of epochs and batch size, you may see accuracy that’s already **better** than our simple logistic regression approach. For more epochs or a more complex architecture, you can push that higher.\n",
        "\n",
        "## 8. Confusion Matrix (Optional)\n",
        "\n",
        "Similar to Day 2, you might want to generate a **confusion matrix** to see how the model performs across classes.\n",
        "\n",
        "### Prompt Example\n",
        "```\n",
        "Generate code to compute a confusion matrix for the test set,\n",
        "store predictions and labels, then visualize with matplotlib.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7058b1a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== OPTIONAL: Confusion Matrix ======\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(cm, cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f8e335",
      "metadata": {},
      "source": [
        "## 9. Observations & Clinical Relevance\n",
        "\n",
        "- **CNN Performance**: Even a small CNN can outperform classical models (like logistic regression on flattened pixels) for image tasks.\n",
        "- **Medical Tie-In**: Automated chest X-ray analysis has huge potential—though real-world models typically use larger images, more advanced architectures, and extensive data.\n",
        "- **Next Steps**: If time permits, try advanced techniques (e.g., more layers, data augmentation, or transfer learning from a bigger pretrained model like ResNet).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4cd01d4",
      "metadata": {},
      "source": [
        "## 10. Assignment #3: Extend & Experiment\n",
        "\n",
        "**Task**:\n",
        "1. Increase the number of epochs (e.g., 5 or 10) and observe if accuracy improves.\n",
        "2. Explore **data augmentations** (random flips or rotations) to see if the model generalizes better.\n",
        "3. Create a simple function to **visualize** a few **model predictions** vs. **true labels** on the test set.\n",
        "\n",
        "### Bonus\n",
        "- Try **transfer learning** with a pretrained CNN (e.g., ResNet-18) by resizing images to 224x224. This may require more HPC resources.\n",
        "- Reflect in your daily log: How could an automated chest X-ray classifier fit into a clinical workflow? What are the limitations and ethical considerations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0733204a",
      "metadata": {},
      "source": [
        "# End of Day 3 Notebook\n",
        "\n",
        "Today, you learned how to:\n",
        "- Load a **medical** imaging dataset (ChestMNIST) suitable for CNNs.\n",
        "- Define and train a **Convolutional Neural Network** in **PyTorch**.\n",
        "- Evaluate its performance and interpret results (accuracy, confusion matrix).\n",
        "\n",
        "**Keep your daily log** updated with your progress, challenges, and reflections.\n",
        "\n",
        "Happy CNN-ing! We’ll continue exploring advanced topics in the upcoming sessions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
